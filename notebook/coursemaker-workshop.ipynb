{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursemaker Trial Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used 100 words in a language make up approximately 50% of words in a corpus (This of course varies from language to language. E.g. it is lover in agglutinative languages). If you sort the most used words and start learning words from this list (frequency sorted list fsl) you can effectively increase your hit rate in the corpus.\n",
    "\n",
    "I want to sort sentences in a similar manner. The first sentence of the proposed order shall ideally contain the first n words in the fsl. Any sentence can contain any number of words from previous sentences, and tries to incorporate any number of next m most used words.\n",
    "\n",
    "The 'vocabulary expansion per words studied' curve is ideally the same as the curve given above and it can theoretically (and practically) never exceed this curve. To achieve this, no mth sentence shall use the n+1th word in the sorted list if the first m sentences don't contain any of the first n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    f = open(path, 'r')\n",
    "    c = f.read()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a string\n",
    "def normalize(s, case_folding=True, stopword_removal=True, punctuation_removal=True, newline_removal=True, punctuation_whitelist=[]):\n",
    "\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # set lowercase\n",
    "    if case_folding:\n",
    "        s = s.lower()\n",
    "\n",
    "    # remove custom html characters and tabs\n",
    "    s = re.sub(r\"&[a-z]{1,3};\", \" \", s)\n",
    "    s = s.replace('\\t', '')\n",
    "\n",
    "    # replace punctuation marks with a blank space\n",
    "    if punctuation_removal:\n",
    "        for character in list(string.punctuation) + ['”', '“']:\n",
    "            if character not in punctuation_whitelist:\n",
    "                s = s.replace(character, ' ')\n",
    "\n",
    "    # remove newline characters ('\\n')\n",
    "    if newline_removal:\n",
    "        s = s.replace('\\n', ' ')\n",
    "\n",
    "    # remove stop words given in stopwords.txt from the string\n",
    "    if stopword_removal:\n",
    "        b = open('stopwords.txt')\n",
    "        stop_words = [ line[:-1] for line in b.readlines() ]\n",
    "        b.close()\n",
    "\n",
    "        for word in stop_words:\n",
    "            s = re.sub(r\" {} \".format(word), \" \", s)\n",
    "            s = re.sub(r\"^{} \".format(word), \" \", s)\n",
    "            s = re.sub(r\" {}$\".format(word), \" \", s)\n",
    "\n",
    "    # shorten mutliple blank spaces into one\n",
    "    s = re.sub(r\" +\", \" \", s)\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the dataset \n",
    "I chose A Tale of Two Cities as a corpora while conducting my experiments. I normalized the corpus and divided it into sentences. I extracted 7481 sentences, which contain 9940 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = readfile('data/twocities.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = normalize(corpus, stopword_removal=False, punctuation_whitelist=['!', '.', '?', '\\''])\n",
    "\n",
    "corpus = corpus.replace('mrs.', 'mrs')\n",
    "corpus = corpus.replace('mr.', 'mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start      = 'the footsteps die out for'\n",
    "startindex = corpus.index(start)\n",
    "\n",
    "sentences  = re.split('\\.|\\?|\\!', corpus[startindex:])\n",
    "\n",
    "words      = normalize(corpus, stopword_removal=False)[startindex:].split(' ')\n",
    "words      = [ word for word in words if re.match('[a-z]+', word) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "All words are counted and sorted according to their frequencies/counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words, counts_words = np.unique(words, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = []\n",
    "for word, count in zip(unique_words, counts_words):\n",
    "    frequencies.append([word, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies.sort(key = lambda x: -x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.array(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_cumulative = [0]\n",
    "for freq in frequencies[:,1].astype(int):\n",
    "    freqs_cumulative.append(freqs_cumulative[-1] + freq)\n",
    "    \n",
    "wcount = freqs_cumulative[-1]\n",
    "\n",
    "freqs_cumulative = np.array(freqs_cumulative[1:])/wcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, figsize=(16,9))\n",
    "\n",
    "axs.plot(freqs_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at the words used in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud(background_color='white', width=1000, height=500)\n",
    "plt.figure(figsize=(19,9))\n",
    "plt.imshow(wc.generate(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_feature(sentences, feature, frequencies):\n",
    "    all_orders = []\n",
    "    \n",
    "    freq_list = list(frequencies[:,0])\n",
    "    \n",
    "    if feature == 'orders':\n",
    "        for sentence in sentences:\n",
    "            sentence_orders = [ freq_list.index(word) for word in sentence.split(' ') if word in freq_list]\n",
    "            all_orders.append(sentence_orders)\n",
    "        \n",
    "        return all_orders\n",
    "    \n",
    "    if feature == 'frequencies':\n",
    "        freqs = np.array(frequencies[:,1]).astype(float)/wcount\n",
    "        for sentence in sentences:\n",
    "            sentence_orders = [ freqs[freq_list.index(word)] for word in sentence.split(' ') if word in freq_list]\n",
    "            all_orders.append(sentence_orders)\n",
    "        \n",
    "        return all_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_orders      = sentences_to_feature(list(sentences), 'orders', frequencies)\n",
    "sentence_frequencies = sentences_to_feature(list(sentences), 'frequencies', frequencies)\n",
    "\n",
    "sentence_orders[0], sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated\n",
    "def get_ideal_index(sentences, frequencies):\n",
    "    \n",
    "    xfrequencies = np.copy(frequencies)\n",
    "    #print('xfreq', xfrequencies[:10])\n",
    "    \n",
    "    xwords       = list(xfrequencies[:,0])\n",
    "    \n",
    "    #print('xwords', xwords[:10])\n",
    "    \n",
    "    xsentences   = [ sentence.split(' ') for sentence in sentences.copy() ]\n",
    "    #print('sentences', xsentences[:4])\n",
    "    xsentences   = [ [token for token in sentence if token in xwords] for sentence in xsentences if sentence != [] ]\n",
    "    xsentences   = [ sentence for sentence in xsentences if sentence != []]\n",
    "    #print('sentences', xsentences)\n",
    "    \n",
    "    for word in xwords:\n",
    "        for i, sentence in enumerate(xsentences):\n",
    "            for token in sentence:\n",
    "                #print('TS:', token, sentence)\n",
    "                if token == word:\n",
    "                    sentence.remove(token)\n",
    "                    \n",
    "            if sentence == []:\n",
    "                #print(i)\n",
    "                return i\n",
    "    \n",
    "    return -1\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated\n",
    "def get_ideal_index_max(sentences, frequencies):\n",
    "    \n",
    "    xfrequencies = np.copy(frequencies)\n",
    "    \n",
    "    xwords       = list(xfrequencies[:,0])\n",
    "\n",
    "    xsentences = [' '.join(list(set(sentence.split(' ')))) for sentence in sentences ]\n",
    "    xsentence_frequencies = sentences_to_feature(list(xsentences), 'frequencies', frequencies)\n",
    "\n",
    "    sums = [ np.sum(xsf) for xsf in xsentence_frequencies ]\n",
    "\n",
    "    return np.argmax(sums)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_index_max_avg(sentences, frequencies):\n",
    "    \n",
    "    xfrequencies = np.copy(frequencies)\n",
    "    \n",
    "    xwords       = list(xfrequencies[:,0])\n",
    "\n",
    "    xsentences = [' '.join([token for token in list(set(sentence.split(' '))) if token in xwords ]) for sentence in sentences ]\n",
    "    xsentence_frequencies = sentences_to_feature(list(xsentences), 'frequencies', frequencies)\n",
    "\n",
    "    sums = [ np.sum(xsf)/len(xsf) if xsf != [] else 0 for xsf in xsentence_frequencies ]\n",
    "\n",
    "    return np.argmax(sums)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_order(sentences, frequencies, sentence_count, metric=\"vanilla\"):\n",
    "    \n",
    "    sentences_ordered = []\n",
    "\n",
    "    remaining_sentences   = [ sentence for sentence in set(sentences.copy()) if len(sentence)>5 ]\n",
    "    remaining_frequencies = np.copy(frequencies)\n",
    "\n",
    "    print(len(remaining_sentences), len(remaining_frequencies))\n",
    "    \n",
    "    cumulative_return = 0\n",
    "\n",
    "    vocab = []\n",
    "    learning_history = [0]\n",
    "\n",
    "    for i in range(sentence_count):\n",
    "\n",
    "        if metric == \"vanilla\":\n",
    "            chosen_index = 1\n",
    "\n",
    "        elif metric == \"def\":\n",
    "            chosen_index = get_ideal_index(remaining_sentences, remaining_frequencies)\n",
    "\n",
    "        elif metric == \"max\":\n",
    "            chosen_index = get_ideal_index_max(remaining_sentences, remaining_frequencies)\n",
    "\n",
    "        elif metric == \"max-avg\":\n",
    "            chosen_index = get_ideal_index_max_avg(remaining_sentences, remaining_frequencies)\n",
    "        \n",
    "        else:\n",
    "            print(\"error: invalid metric\")\n",
    "            return None\n",
    "        \n",
    "        if chosen_index == -1:\n",
    "            print(\"error\")\n",
    "            return None\n",
    "\n",
    "        sentence = remaining_sentences.pop(chosen_index)\n",
    "\n",
    "        newvocab = [ word for word in set(sentence.split(' ')) if word not in vocab ]\n",
    "        filtered = ' '.join(newvocab)\n",
    "\n",
    "        orders   = sentences_to_feature(list([filtered]), 'orders', remaining_frequencies)\n",
    "        sfreqs   = sentences_to_feature(list([filtered]), 'frequencies', remaining_frequencies)\n",
    "\n",
    "        vocab   += newvocab\n",
    "\n",
    "        new_percentage = 100*np.sum(sfreqs) if sfreqs else 0\n",
    "        cumulative_return += new_percentage\n",
    "\n",
    "        trunc = 100\n",
    "        print('{} - return: {:.2f}% ({:.2f}% cumulative)'.format(i, new_percentage, cumulative_return),\n",
    "              f'\\n{chosen_index}:\\t\"{sentence[:trunc]}{\"...\"*int(len(sentence)>trunc)}\"', '\\n')\n",
    "\n",
    "        learning_history.append(learning_history[-1] + new_percentage)\n",
    "        sentences_ordered.append(sentence)\n",
    "\n",
    "        for order in orders:\n",
    "            remaining_frequencies = np.delete(remaining_frequencies, order, 0)\n",
    "            \n",
    "        if not remaining_sentences:\n",
    "            break\n",
    "            \n",
    "    plt.plot(learning_history)\n",
    "    \n",
    "    return sentences_ordered, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_in_order(sentences, frequencies, 5, metric=\"max-avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_count(sentences):\n",
    "    counts = [0]\n",
    "    vocabs = [set([]), ]\n",
    "    for sentence in sentences:\n",
    "        tokens = [ token for token in set(sentence.split(' ')) if token in words ]\n",
    "        counts.append(len)\n",
    "        vocabs.append(set(list(vocabs[-1])+tokens))\n",
    "    return counts, vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
